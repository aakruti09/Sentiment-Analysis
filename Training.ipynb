{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf87ef9",
   "metadata": {
    "id": "9cf87ef9"
   },
   "source": [
    "# Sentiment Analysis\n",
    "#### **Submitted by : Aakruti Ambasana, Akshay Sharma, Rahul Dingria**\n",
    "#### **Group : 27**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b67820",
   "metadata": {
    "executionInfo": {
     "elapsed": 2198,
     "status": "ok",
     "timestamp": 1626719146576,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "b7b67820"
   },
   "outputs": [],
   "source": [
    "# Importing libraries that are needed\n",
    "import os\n",
    "import string\n",
    "import shutil\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d115a1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29198,
     "status": "ok",
     "timestamp": 1626719175766,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "0d115a1e",
    "outputId": "769e3595-3e7a-426a-b054-45bd61cd2086"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the Current directory path and accessing the training dataset from 'aclImdb'.\n",
    "# Relative path is specified.\n",
    "current_path = os.getcwd()\n",
    "path1 = os.path.join(current_path,\"aclImdb\")\n",
    "train_folder=os.path.join(path1,\"train\")\n",
    "os.listdir(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791c3257",
   "metadata": {
    "executionInfo": {
     "elapsed": 1390,
     "status": "ok",
     "timestamp": 1626719177149,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "791c3257"
   },
   "outputs": [],
   "source": [
    "# Removed unsupervised folder from train folder of aclImdb because it is supervised problem \n",
    "# and there is no need of unsupervised data.\n",
    "remove_dir = os.path.join(train_folder, 'unsup')\n",
    "try:\n",
    "    shutil.rmtree(remove_dir)\n",
    "except OSError as e:\n",
    "    print(\"'unsup' folder does not exist :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bf2f0",
   "metadata": {
    "id": "6e0bf2f0"
   },
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ecc6df",
   "metadata": {
    "id": "b0ecc6df"
   },
   "source": [
    "- Extracting data from each text file. The default batch size is 32. The seed value is set to 9 for validation set, training set and test set. The seed value is specified and same because there should be no overlap between the training set and validation set. \n",
    "- text_dataset_from_directory() will return the BatchDataset containing movie review and label tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd846928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4296,
     "status": "ok",
     "timestamp": 1626719181433,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "dd846928",
    "outputId": "b07d4919-a7b1-46b5-f391-8bef7f19cc7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "Label 0 is neg\n",
      "Label 1 is pos\n"
     ]
    }
   ],
   "source": [
    "data=tf.keras.preprocessing.text_dataset_from_directory('aclImdb/train', seed=9,\n",
    "    validation_split=0.2,subset=\"training\")\n",
    "val_data=tf.keras.preprocessing.text_dataset_from_directory('aclImdb/train', seed=9,\n",
    "    validation_split=0.2,subset=\"validation\")\n",
    "test_data=tf.keras.preprocessing.text_dataset_from_directory('aclImdb/test')\n",
    "print(data)\n",
    "# Considering label 0 as negative and label 1 as positive movie review\n",
    "print('Label 0 is',data.class_names[0])\n",
    "print('Label 1 is',data.class_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f623bb",
   "metadata": {
    "id": "44f623bb"
   },
   "source": [
    "#### Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746251a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1626719182038,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "746251a2"
   },
   "outputs": [],
   "source": [
    "def clean_text(input_data):\n",
    "    # Converting each word to lower case because vocabulary is also in lower case in 'imdb.vocab'\n",
    "    lower = tf.strings.lower(input_data)\n",
    "    # Removing new line characters from movie reviews\n",
    "    space = tf.strings.regex_replace(lower, \"<br />\", \" \")\n",
    "    # Removing punctuations from sentences \n",
    "    clean_punch = tf.strings.regex_replace(space, '[%s]' %re.escape(string.punctuation), '')\n",
    "    return clean_punch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f0ba5",
   "metadata": {
    "id": "c35f0ba5"
   },
   "source": [
    "#### Tokenizing the dataset\n",
    "\n",
    "* Text Vectorization is for converting text into numerical number. That means if the word in vocab \n",
    "like \"home\" is in 339 position. So TextVectorization will assign each word with numbers. \n",
    "* In this, output_sequence_length is specified as 500 because all the movie reviews will have different number of words, this will truncate the review with words greater than 500 and perform padding for review which have words less than 500. \n",
    "* clean_text() is called from standardize which will clean the text of dataset. Then each word is tokenized and mapped to the integer number. The vocabulary is used from 'aclImdb/imdb.vocab' \n",
    "* Output_mode is set to integer number will be assigned to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9bd51bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1626719182376,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "a9bd51bf",
    "outputId": "c7326422-ded9-4fa0-a78b-23e4c99ac4cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word at number 339 : home\n",
      "Word at number 56863 : eco\n",
      "Vocabulary size of file imdb.vocab :  89529\n"
     ]
    }
   ],
   "source": [
    "numerical_text_layer = TextVectorization(standardize=clean_text, vocabulary='aclImdb/imdb.vocab',\n",
    "    output_mode='int', output_sequence_length=500)\n",
    "# print(numerical_text_layer)\n",
    "print(\"Word at number 339 :\",numerical_text_layer.get_vocabulary()[339])\n",
    "print(\"Word at number 56863 :\",numerical_text_layer.get_vocabulary()[56863])\n",
    "vocab_size = len(numerical_text_layer.get_vocabulary())\n",
    "print('Vocabulary size of file imdb.vocab : ',vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97797335",
   "metadata": {
    "id": "97797335"
   },
   "source": [
    "##### Checking one movie review, to see mapping of text into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bd1edd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1626719182730,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "6bd1edd4",
    "outputId": "e981306c-93b6-42d5-9c67-d8559452878f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "Batch size of data: 32\n",
      "Movie Review: tf.Tensor(b\"OK maybe a 13 year old like me was a little to old for this movie. Its about this pampered rat, who lives in a palace. Then a sewer rat flushes him down a toilet! He ends up in this rat city and meets this girl rat who has a gem a greedy frog wants. He will do anything for this gem he sends a whole army after these two rats.He plans to take the gem and to flood rat city! THe cool part about this movie is the slugs. They do all the sound effects. They sing, make noises, its awesome, its also pretty funny. OK bottom line, it is aimed at 7 year olds. Other wise, a great movie to take a younger family member to see. I didn't think the animation was real dreamworks art though, more like WAllace and Gromit. i thinkthey slacked a little on that. The movie was just decent, not worth spending $9.50 for though, sorry.\", shape=(), dtype=string)\n",
      "Movie Review Label: neg\n",
      "Vectorized review (<tf.Tensor: shape=(1, 500), dtype=int64, numpy=\n",
      "array([[  585,   268,     4,     1,   319,   166,    38,    70,    13,\n",
      "            4,   114,     6,   166,    15,    11,    17,    91,    43,\n",
      "           11, 23319,  4521,    34,   444,     9,     4,  4831,    93,\n",
      "            4, 12096,  4521, 27021,    88,   185,     4,  3408,    23,\n",
      "          610,    58,     9,    11,  4521,   521,     3,   880,    11,\n",
      "          238,  4521,    34,    45,     4,  1465,     4,  4497,  6915,\n",
      "          475,    23,    78,    83,   226,    15,    11,  1465,    23,\n",
      "         3203,     4,   219,  1228,   100,   132,   107,     1,  2377,\n",
      "            6,   190,     2,  1465,     3,     6,  9962,  4521,   521,\n",
      "            2,   633,   172,    43,    11,    17,     7,     2,  6550,\n",
      "           35,    83,    30,     2,   471,   301,    35,  1970,    96,\n",
      "         5157,    91,  1145,    91,    81,   180,   160,   585,  1342,\n",
      "          359,     8,     7,  3648,    31,     1,   319,  8558,    80,\n",
      "         2035,     4,    85,    17,     6,   190,     4,  1113,   211,\n",
      "         1613,     6,    66,    10,  9088,   102,     2,   723,    13,\n",
      "          147, 11479,   513,   148,    51,    38,  3823,     3, 21143,\n",
      "           10, 59956, 87968,     4,   114,    20,    12,     2,    17,\n",
      "           13,    42,   527,    21,   283,  3330,     1,    15,   148,\n",
      "          770,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "\n",
    "def print_numerical_text(text, label):\n",
    "    # Converting text tensor shape from () to (1,) because of numerical_text_layer.\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    # print(text.shape)\n",
    "    # Applied vocabulary on text data and returned it the numerical text and label.\n",
    "    return numerical_text_layer(text), label\n",
    "\n",
    "text_batch, label_batch = next(iter(data))\n",
    "# It cleans the dataset\n",
    "# clean_batch = clean_text(text_batch)\n",
    "# print('Cleaned:',clean_batch)\n",
    "\n",
    "print(\"Batch size of data:\",len(text_batch))\n",
    "review, lbl = text_batch[2], label_batch[2]\n",
    "print(\"Movie Review:\", review)\n",
    "print(\"Movie Review Label:\", data.class_names[lbl])\n",
    "print(\"Vectorized review\", print_numerical_text(review, lbl))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027d773",
   "metadata": {
    "id": "4027d773"
   },
   "source": [
    "##### Converted whole training dataset, validation dataset and test dataset reviews in numerical numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c2e32a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1626719183246,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "9c2e32a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this, training data, validation data and test data is mapped each word of review into integer.\n",
    "training_data = data.map(print_numerical_text)\n",
    "validation_data = val_data.map(print_numerical_text)\n",
    "test_data = test_data.map(print_numerical_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8a8aa",
   "metadata": {
    "id": "56b8a8aa"
   },
   "source": [
    "### Created and trained model (First Artificial Neural Network)\n",
    "I have tried a simple neural network with 3 layers.\n",
    "First layer is embedding layer, Second layer is GlobalAveragePooling1D(), Third layer is output layer and activation function selected is sigmoid function because for binary classification sigmoid function is a good choice. Binary Cross Entropy is used as loss function.  \n",
    "The EarlyStopping is used to avoid overfitting. In Early Stopping method, Validation loss is monitored. The training set should be trained until validation loss is decreasing but when validation loss increases then training will be stopped. The patience specified is 3. The patience means it will wait for 3 epochs if the validation loss increases for 3 epochs then the training will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TkDxZfjkOakT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 434588,
     "status": "ok",
     "timestamp": 1626719617813,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "TkDxZfjkOakT",
    "outputId": "3c1c5844-17f9-4c58-a519-20fcea738f79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 16)           1432464   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,432,481\n",
      "Trainable params: 1,432,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "625/625 [==============================] - 145s 227ms/step - loss: 0.6861 - accuracy: 0.5809 - val_loss: 0.6394 - val_accuracy: 0.7214\n",
      "Epoch 2/50\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6094 - accuracy: 0.7771 - val_loss: 0.5331 - val_accuracy: 0.8140\n",
      "Epoch 3/50\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.5018 - accuracy: 0.8332 - val_loss: 0.4541 - val_accuracy: 0.8452\n",
      "Epoch 4/50\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.4195 - accuracy: 0.8674 - val_loss: 0.3990 - val_accuracy: 0.8634\n",
      "Epoch 5/50\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.3609 - accuracy: 0.8837 - val_loss: 0.3634 - val_accuracy: 0.8700\n",
      "Epoch 6/50\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.3176 - accuracy: 0.8947 - val_loss: 0.3381 - val_accuracy: 0.8748\n",
      "Epoch 7/50\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.2853 - accuracy: 0.9057 - val_loss: 0.3208 - val_accuracy: 0.8784\n",
      "Epoch 8/50\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.2581 - accuracy: 0.9155 - val_loss: 0.3078 - val_accuracy: 0.8816\n",
      "Epoch 9/50\n",
      "625/625 [==============================] - 288s 461ms/step - loss: 0.2364 - accuracy: 0.9246 - val_loss: 0.2984 - val_accuracy: 0.8852\n",
      "Epoch 10/50\n",
      " 68/625 [==>...........................] - ETA: 3:30 - loss: 0.2295 - accuracy: 0.9254"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "ann1 = tf.keras.Sequential([\n",
    "  layers.Embedding(vocab_size, embedding_dim, input_length=500),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(1, activation = 'sigmoid')])\n",
    "ann1.summary()\n",
    "ann1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "epochs = 50\n",
    "ann1.fit(training_data, validation_data=validation_data, epochs=epochs, \n",
    "                  callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])\n",
    "# ann1loss, ann1accuracy = ann1.evaluate(test_data)\n",
    "# print(\"Loss of Test dataset: \", ann1loss)\n",
    "# print(\"Accuracy of Test dataset: \", ann1accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HZi5m-7WVY2p",
   "metadata": {
    "id": "HZi5m-7WVY2p"
   },
   "source": [
    "### Second Artificial Neural Network\n",
    "* For better accuracy, Neural Network with Dropout layers is added to improve generalization and another dense layer is added for better result. In the Dropout layers, 20% neurons are not used and 80% neurons data is passed to next layer.\n",
    "* In the Dense layer 16 neurons are used because the embedding size is 16. I have also tried with multiple layers and 8 neurons in hidden layer. But performance of that models is less than the model specified below in terms of loss and accuracy. \n",
    "* To avoid overfitting, the EarlyStopping is used with the patience of 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30d9b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175794,
     "status": "ok",
     "timestamp": 1626719802451,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "8f30d9b9",
    "outputId": "526a5757-53cf-4c93-b14d-b1d515c95db9"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "ann = tf.keras.Sequential([\n",
    "  layers.Embedding(vocab_size, embedding_dim, input_length=500),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(16, activation = 'relu'),\n",
    "  layers.Dense(1, activation = 'sigmoid')])\n",
    "ann.summary()\n",
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "epochs = 50\n",
    "history = ann.fit(training_data, validation_data=validation_data, epochs=epochs, \n",
    "                  callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])\n",
    "# loss, accuracy = ann.evaluate(test_data)\n",
    "# print(\"Loss of Test dataset: \", loss)\n",
    "# print(\"Accuracy of Test dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cVgT0EWa-U",
   "metadata": {
    "id": "64cVgT0EWa-U"
   },
   "source": [
    "### Third CNN\n",
    "* For better results, Convolution Neural Network for Natural Language Processing is tried. In this, one convolution layer is used and one max pooling layer is used. And rest of the configurations are same as above in CNN. I have tried multiple convolution layers with multiple max pooling layer but the performance of that approaches are not good than this approach specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718f19a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189323,
     "status": "ok",
     "timestamp": 1626720000678,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "8718f19a",
    "outputId": "7ccf8fdf-495b-40f7-d186-046785c78b37"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.Sequential([\n",
    "  layers.Embedding(vocab_size, embedding_dim, input_length=500),\n",
    "  layers.Conv1D(16, 2, activation='relu'),\n",
    "  layers.MaxPool1D(2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation = 'relu'),\n",
    "  layers.Dense(1, activation = 'sigmoid')])\n",
    "cnn.summary()\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "epochs = 50\n",
    "chistory = cnn.fit(training_data, validation_data=validation_data, epochs=epochs, \n",
    "                  callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])\n",
    "closs, caccuracy = cnn.evaluate(test_data)\n",
    "print(\"Loss of Test dataset: \", closs)\n",
    "print(\"Accuracy of Test dataset: \", caccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qc06maaGZamD",
   "metadata": {
    "id": "qc06maaGZamD"
   },
   "source": [
    "From all three model, the second model gives the best result because I generalizes the model, by adding Dropout layer and number of epochs is less than first layer. In second layer number of epochs is 7 whereas, in First model, number of epochs is 19. As the number of epochs increases the execution time also increases. The execution time for cnn model is more because of convolution layer. It takes more time. So the execution time in second model is minimum from all three models tried.  \n",
    "The comparison of 3 models performance:\n",
    "\n",
    "|  | Number of Epochs | Train Accuracy | Validation Accuracy | Train Loss | Validation Loss |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| ANN with 3 layers (First ANN) | 19 | 97% | 89% | 0.10 | 0.27 |\n",
    "| ANN with 6 layers (Second ANN) | 7 | 97% | 89% | 0.09 | 0.30 |\n",
    "| CNN with 6 layers | 5 | 98% | 89% | 0.05 | 0.36 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chMlPFm_aYbu",
   "metadata": {
    "id": "chMlPFm_aYbu"
   },
   "source": [
    "**Checking the word embedding, which is learned after the training of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de176677",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1626720000679,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "de176677",
    "outputId": "ecf286a7-46c8-4b31-e858-32b4dc4e808d"
   },
   "outputs": [],
   "source": [
    "# Embedding learned \n",
    "first_layer=ann.layers[0]\n",
    "word_embeddings=first_layer.get_weights()[0]\n",
    "print(word_embeddings.shape)\n",
    "print(\"Word embedding of word 'home'\")\n",
    "print(word_embeddings[339][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8365d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4703,
     "status": "ok",
     "timestamp": 1626720005376,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "df8365d9",
    "outputId": "2247757a-b679-492f-a136-dd44234644b9"
   },
   "outputs": [],
   "source": [
    "!pip install h5py pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vk1YAAvjPy9C",
   "metadata": {
    "id": "vk1YAAvjPy9C"
   },
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28200d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1626720005376,
     "user": {
      "displayName": "Aakruti Ambasana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWRRPX1u1DPGoXXy-cT-cI7S04ovgkrw6HGqWC=s64",
      "userId": "16676163511182747311"
     },
     "user_tz": -330
    },
    "id": "f28200d8"
   },
   "outputs": [],
   "source": [
    "ann.save('models/20912881_NLP_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b1dd4",
   "metadata": {},
   "source": [
    "#### References:\n",
    "- https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "- https://nptel.ac.in/courses/106/106/106106213/\n",
    "- https://www.tensorflow.org/tutorials/keras/save_and_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa4508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "q3_sentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
